{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e68f029-7c5f-4b87-b5ed-b57e5ea40b70",
   "metadata": {},
   "source": [
    "# Lunar Lander \n",
    "\n",
    "In this notebook, we will train an agent using a Deep Q-Network (DQN) to land a spacecraft safely in the LunarLander-v3 environment.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [1. Project Overview](#1)\n",
    "- [2. About the Environment](#2)\n",
    "- [3. Imports and Setup](#3)\n",
    "- [4. Build the Q-Network](#4)\n",
    "- [5. Build the Replay Buffer](#5)\n",
    "- [6. Build the DQN Agent](#6)\n",
    "- [7. Train the Agent](#7)\n",
    "- [8. Main Training Execution](#8)\n",
    "- [9. Visualize Training Progress](#9)\n",
    "- [10. Results](#10)\n",
    "- [11. Interpretation and Conclusion](#11)\n",
    "- [12. References](#12)\n",
    "\n",
    "---\n",
    "\n",
    "<a id='1'></a>\n",
    "## 1. Project Overview\n",
    "\n",
    "Here, we’ll build an agent that learns how to land a spacecraft safely on the moon.\n",
    "The goal is to test how well a Deep Q-Network (DQN) can figure out how to land using only rewards and trial-and-error.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='2'></a>\n",
    "\n",
    "## 2. About the Environment\n",
    "\n",
    "The LunarLander-v3 environment simulates a spacecraft trying to land safely on a designated landing pad located at coordinates (0,0).\n",
    "We are using the default settings provided by Gymnasium to keep the task manageable but still challenging:\n",
    "- Action Type: Discrete (4 possible moves)\n",
    "- Wind: Disabled (no random gusts)\n",
    "- Gravity: Set to -10.0\n",
    "\n",
    "\n",
    "**Action Space**\n",
    "\n",
    "The agent can choose between 4 possible discrete actions:\n",
    "\n",
    "- 0: Do nothing\n",
    "- 1: Fire left orientation engine\n",
    "- 2: Fire main engine\n",
    "- 3: Fire right orientation engine\n",
    "    \n",
    "**State Space**\n",
    "\n",
    "The environment provides an 8-dimensional observation vector describing the current situation:\n",
    "- X and Y coordinates (position)\n",
    "- X and Y velocities (movement speed)\n",
    "- Lander angle (tilt)\n",
    "- Angular velocity (rate of rotation)\n",
    "- Left leg contact (boolean)\n",
    "- Right leg contact (boolean)<br>\n",
    "\n",
    "\n",
    "**Rewards**\n",
    "\n",
    "Every step of the simulation, the agent earns a reward based on how it behaves\n",
    "\n",
    "| **Condition**                     | **Reward Impact**            |\n",
    "|:----------------------------------|:------------------------------|\n",
    "| Moving closer to landing pad      | Increases reward              |\n",
    "| Moving further from landing pad   | Decreases reward              |\n",
    "| Moving slower                     | Increases reward              |\n",
    "| Moving faster                     | Decreases reward              |\n",
    "| Tilting away from horizontal      | Decreases reward              |\n",
    "| Touching ground with a leg         | +10 points per leg            |\n",
    "| Using side engine (each frame)     | -0.03 points                  |\n",
    "| Using main engine (each frame)     | -0.3 points                   |\n",
    "    \n",
    "**The Goal**\n",
    "\n",
    "We will consider the agent successful if it achieves an average score of **200 points** or higher across multiple episodes.\n",
    "\n",
    "  \n",
    "---\n",
    "\n",
    "<a id='3'></a>\n",
    "\n",
    "## 3. Imports and Setup\n",
    "Time to gather our supplies and tools for this project.\n",
    "Load required packages and check if we can use a GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e6c7d9-21c1-4772-9ae0-1a30c5c67760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# Make sure we run on GPU if available (otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38a707-b379-4e1b-9f7d-380fc0f9c17b",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='4'></a>\n",
    "\n",
    "## 4. Build the Q-Network\n",
    "\n",
    "Think of this as building the “brain” of our lander.\n",
    "It looks at the situation and tries to predict what move would be the smartest.\n",
    "- Looks at the current situation (speed, angle, distance)\n",
    "- Tries to guess which move (action) will help us land safely\n",
    "- Gets better over time by learning from rewards\n",
    "\n",
    "We’ll build a simple neural network — just a few math layers stacked together — that can learn to make smart decisions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e92c256-66f8-4bba-9ea0-74330180c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1592a-acd9-4338-bf27-faf078f5ae88",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='5'></a>\n",
    "## 5. Build the Replay Buffer\n",
    "This is our agent’s memory.\n",
    "It saves what happened (good and bad) so the agent can go back and learn from it later.\n",
    "\n",
    "Without memory, the agent would forget everything immediately after each move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685a45d2-525b-439f-9110-7239196d8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple(\"Experience\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(self.experience(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = torch.from_numpy(np.vstack([e.state for e in batch])).float().to(device)\n",
    "        actions     = torch.from_numpy(np.vstack([e.action for e in batch])).long().to(device)\n",
    "        rewards     = torch.from_numpy(np.vstack([e.reward for e in batch])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in batch])).float().to(device)\n",
    "        dones       = torch.from_numpy(np.vstack([e.done for e in batch]).astype(np.uint8)).float().to(device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904768a-1a53-4f4f-bab9-47d499184bd8",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='6'></a>\n",
    "## 6. Build the DQN Agent\n",
    "Now we pull everything together!\n",
    "This is the body and mind of our agent.\n",
    "\n",
    "The agent will:\n",
    "- Use the Q-network (brain) to pick actions\n",
    "- Use the replay buffer (memory) to save experiences\n",
    "- Learn from past mistakes to improve over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b4a76a-6c56-47f9-979b-e52cdfbe1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-4, gamma=0.99, tau=1e-3, batch_size=64, update_every=5):\n",
    "        self.q_eval = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.q_target = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_eval.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.step_counter = 0\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def act(self, state, epsilon=0.1):\n",
    "        # Epsilon-greedy: sometimes explore, sometimes exploit\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.q_eval.layers[-1].out_features)\n",
    "        else:\n",
    "            state_t = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                return int(torch.argmax(self.q_eval(state_t)).item())\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience into memory\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every few steps\n",
    "        self.step_counter = (self.step_counter + 1) % self.update_every\n",
    "        if self.step_counter == 0 and len(self.buffer) >= self.batch_size:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample a batch from memory\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        # Calculate targets\n",
    "        q_targets_next = self.q_target(next_states).detach().max(dim=1, keepdim=True)[0]\n",
    "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Calculate predictions\n",
    "        q_expected = self.q_eval(states).gather(1, actions)\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss = self.loss_fn(q_expected, q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Slowly update the target network\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, eval_param in zip(self.q_target.parameters(), self.q_eval.parameters()):\n",
    "            target_param.data.copy_(self.tau * eval_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988e13-92f9-4333-a1e9-5f296f7d37dd",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='7'></a>\n",
    "## 7. Train the Agent\n",
    "This is like sending the agent to flight school.\n",
    "It practices landing over and over, learns from crashes, and slowly gets better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72fe23d-5a20-435a-a255-ff698969b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, n_episodes=1500, max_steps=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    # This will hold the total reward earned in each episode\n",
    "    scores = []\n",
    "    epsilon = eps_start  # Start with a high exploration rate\n",
    "\n",
    "    for ep in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset(seed=ep)  # Start a new episode\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)  # Pick an action (either explore or exploit)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # Do the action\n",
    "            done = terminated or truncated  # Check if episode finished\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)  # Save the experience and maybe learn\n",
    "\n",
    "            state = next_state  # Move to the next state\n",
    "            total_reward += reward  # Add up rewards for this episode\n",
    "\n",
    "            if done:\n",
    "                break  # Stop if the episode ends (either crash or land)\n",
    "\n",
    "        scores.append(total_reward)  # Save the total reward for this episode\n",
    "        epsilon = max(eps_end, eps_decay * epsilon)  # Slowly lower epsilon (exploration)\n",
    "\n",
    "        # Save a snapshot of progress at different checkpoints\n",
    "        if ep in [500, 1000, 1500]:  \n",
    "            plt.figure(figsize=(10,6))\n",
    "            plt.plot(np.arange(len(scores)), scores)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Total Reward')\n",
    "            plt.title(f'Training Progress After {ep} Episodes')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"training_curve_{ep}.png\")  # Save it as a PNG file\n",
    "            plt.close()\n",
    "\n",
    "        # Every 100 episodes, show quick updates\n",
    "        if ep % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print(f\"Episode {ep} | Average (last 100 episodes): {avg_score:.2f}\")\n",
    "\n",
    "    return scores  # After all episodes are done, return all the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0085a3-a011-4d04-a423-b88919ccedb5",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='8'></a>\n",
    "## 8. Main Training Execution\n",
    "Here’s where we actually start the flight practice.\n",
    "We’ll let the agent fly thousands of times and keep track of how it’s doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f6261f-6ca4-42b6-ab3a-bc8c0b959d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Average (last 100 episodes): -241.32\n",
      "Episode 200 | Average (last 100 episodes): -251.01\n",
      "Episode 300 | Average (last 100 episodes): -172.78\n",
      "Episode 400 | Average (last 100 episodes): -73.35\n",
      "Episode 500 | Average (last 100 episodes): -65.05\n",
      "Episode 600 | Average (last 100 episodes): -40.48\n",
      "Episode 700 | Average (last 100 episodes): -16.69\n",
      "Episode 800 | Average (last 100 episodes): 1.84\n",
      "Episode 900 | Average (last 100 episodes): -18.06\n",
      "Episode 1000 | Average (last 100 episodes): -64.88\n",
      "Episode 1100 | Average (last 100 episodes): -43.84\n",
      "Episode 1200 | Average (last 100 episodes): -55.00\n",
      "Episode 1300 | Average (last 100 episodes): -15.23\n",
      "Episode 1400 | Average (last 100 episodes): 43.11\n",
      "Episode 1500 | Average (last 100 episodes): 101.90\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "# Train agent\n",
    "scores = train_dqn(env, agent)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(agent.q_eval.state_dict(), \"dqn_lander.pth\")\n",
    "\n",
    "# Close environment after training\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d577c7-45aa-4f5f-b172-1dc9731bd5d7",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='9'></a>\n",
    "\n",
    "## 9. Visualize Training Progress\n",
    "\n",
    "We’ll plot a simple graph to see how the agent’s landing skills improve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa8fc5-2380-437e-9595-23b28b1e7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how the agent's rewards changed over time\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.xlabel('Episode')  # Each \"episode\" is one flight attempt\n",
    "plt.ylabel('Total Reward')  # Higher reward = better landing\n",
    "plt.title('Training Performance Over Episodes')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the final, full training curve too\n",
    "plt.savefig(\"training_curve_final.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5378c89-19e2-4aa2-989c-be7cde6aa8d7",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='10'></a>\n",
    "\n",
    "## 10. Results\n",
    "\n",
    "After training the agent for 1500 episodes, here are the key outcomes:\n",
    "\n",
    "- **Early training:** Rewards were very low and unstable. Most flights ended in crashes or missing the landing pad.\n",
    "- **Mid training (~500-1000 episodes):** Rewards became more stable. The lander started to slow down before landing and balanced better.\n",
    "- **Late training (~1500 episodes):** The agent consistently achieved safe landings with higher rewards.\n",
    "\n",
    "Here are visualizations of the agent's progress during training:\n",
    "\n",
    "- Training progress after 500 episodes:(Training graph inserted here after running)\n",
    "- Training progress after 1000 episodes: (Training graph inserted here after running)\n",
    "- Final training curve after 1500 episodes: (Training graph inserted here after running)\n",
    "---\n",
    "\n",
    "<a id='11'></a>\n",
    "\n",
    "## 11. Interpretation and Conclusion\n",
    "\n",
    "### Interpretation:\n",
    "The Deep Q-Network (DQN) successfully learned how to land safely through reinforcement learning:\n",
    "- It used trial-and-error and rewards to adjust its moves.\n",
    "- It learned to stabilize faster and use engines more carefully.\n",
    "- It balanced between exploring random moves and exploiting smart strategies.\n",
    "\n",
    "The agent's performance clearly improved when comparing early vs. late training curves.\n",
    "\n",
    "### Conclusion:\n",
    "This project shows that even a simple Deep Q-Network, with the right settings and enough training episodes, can solve a challenging control task like Lunar Lander.\n",
    "\n",
    "If we had more time or resources, future improvements could include:\n",
    "- Trying Double DQN or Dueling DQN to improve learning stability\n",
    "- Using Prioritized Experience Replay\n",
    "- Training for more episodes to get even better landings\n",
    "- Testing in environments with wind or turbulence for a harder challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930583a-0c83-4bc7-8960-76fb7604d1b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='12'></a>\n",
    "## 12. References\n",
    "[1] [Gymnasium GitHub Repository](https://github.com/Farama-Foundation/Gymnasium)\n",
    "\n",
    "[2] [Gymnasium LunarLander-v3 Documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "[3] [Deep Reinforcement Learning Resources - Gymnasium Farama Docs](https://gymnasium.farama.org/tutorials/)\n",
    "\n",
    "[4] [PyTorch Reinforcement Learning Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "\n",
    "[5] [Box2D Physics Engine](https://box2d.org/)\n",
    "\n",
    "[6] [Example Project: DQN for LunarLander-v2 by yuchen071 (GitHub)](https://github.com/yuchen071/DQN-for-LunarLander-v2)\n",
    "\n",
    "[7] [Original Lunar Lander Reinforcement Learning Guide by sokistar24](https://github.com/sokistar24/Deep_Reinforcement_learning)\n",
    "\n",
    "[8] [Deep Q-Learning Paper (Mnih et al., 2015)](https://arxiv.org/abs/1312.5602)\n",
    "\n",
    "[9] [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066508d-1de2-47e3-ab40-1d1832847008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
