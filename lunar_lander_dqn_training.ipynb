{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e68f029-7c5f-4b87-b5ed-b57e5ea40b70",
   "metadata": {},
   "source": [
    "# Lunar Lander \n",
    "\n",
    "In this notebook, we will train an agent using a Deep Q-Network (DQN) to land a spacecraft safely in the LunarLander-v3 environment.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [1. Project Overview](#1)\n",
    "- [2. About the Environment](#2)\n",
    "- [3. Imports and Setup](#3)\n",
    "- [4. Build the Q-Network](#4)\n",
    "- [5. Build the Replay Buffer](#5)\n",
    "- [6. Build the DQN Agent](#6)\n",
    "- [7. Train the Agent](#7)\n",
    "- [8. Main Training Execution](#8)\n",
    "- [9. Visualize Training Progress](#9)\n",
    "- [10. Results, Interpretation, and Conclusion](#10)\n",
    "- [11. References](#11)\n",
    "\n",
    "---\n",
    "\n",
    "<a id='1'></a>\n",
    "## 1. Project Overview\n",
    "\n",
    "Here, we’ll build an agent that learns how to land a spacecraft safely on the moon.\n",
    "The goal is to test how well a Deep Q-Network (DQN) can figure out how to land using only rewards and trial-and-error.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='2'></a>\n",
    "\n",
    "## 2. About the Environment\n",
    "\n",
    "The LunarLander-v3 environment simulates a spacecraft trying to land safely at a specific location.\n",
    "\n",
    "**Action Space**: 4 discrete actions\n",
    "  - 0: Do nothing\n",
    "  - 1: Fire left engine\n",
    "  - 2: Fire main engine\n",
    "  - 3: Fire right engine\n",
    "- **State Space**: 8 continuous values describing position, velocity, angle, and leg contact.\n",
    "- **Rewards**:\n",
    "  - Positive rewards for moving closer to the pad, slowing down, and touching down gently.\n",
    "  - Penalties for fuel use, tilting, crashing.\n",
    "- **Success**: An average score of 200+ points.\n",
    "- We use the default settings: discrete actions, no wind, gravity at -10.0.\n",
    "  \n",
    "---\n",
    "\n",
    "<a id='3'></a>\n",
    "\n",
    "## 3. Imports and Setup\n",
    "Time to gather our supplies and tools for this project.\n",
    "Load required packages and check if we can use a GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6c7d9-21c1-4772-9ae0-1a30c5c67760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# Make sure we run on GPU if available (otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38a707-b379-4e1b-9f7d-380fc0f9c17b",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='4'></a>\n",
    "\n",
    "## 4. Build the Q-Network\n",
    "\n",
    "Think of this as building the “brain” of our lander.\n",
    "It looks at the situation and tries to predict what move would be the smartest.\n",
    "- Looks at the current situation (speed, angle, distance)\n",
    "- Tries to guess which move (action) will help us land safely\n",
    "- Gets better over time by learning from rewards\n",
    "\n",
    "We’ll build a simple neural network — just a few math layers stacked together — that can learn to make smart decisions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92c256-66f8-4bba-9ea0-74330180c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1592a-acd9-4338-bf27-faf078f5ae88",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='5'></a>\n",
    "## 5. Build the Replay Buffer\n",
    "This is our agent’s memory.\n",
    "It saves what happened (good and bad) so the agent can go back and learn from it later.\n",
    "\n",
    "Without memory, the agent would forget everything immediately after each move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a45d2-525b-439f-9110-7239196d8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple(\"Experience\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(self.experience(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states      = torch.from_numpy(np.vstack([e.state for e in batch])).float().to(device)\n",
    "        actions     = torch.from_numpy(np.vstack([e.action for e in batch])).long().to(device)\n",
    "        rewards     = torch.from_numpy(np.vstack([e.reward for e in batch])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in batch])).float().to(device)\n",
    "        dones       = torch.from_numpy(np.vstack([e.done for e in batch]).astype(np.uint8)).float().to(device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904768a-1a53-4f4f-bab9-47d499184bd8",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='6'></a>\n",
    "## 6. Build the DQN Agent\n",
    "Now we pull everything together!\n",
    "This is the body and mind of our agent.\n",
    "\n",
    "The agent will:\n",
    "- Use the Q-network (brain) to pick actions\n",
    "- Use the replay buffer (memory) to save experiences\n",
    "- Learn from past mistakes to improve over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4a76a-6c56-47f9-979b-e52cdfbe1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-4, gamma=0.99, tau=1e-3, batch_size=64, update_every=5):\n",
    "        self.q_eval = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.q_target = QNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_eval.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.step_counter = 0\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def act(self, state, epsilon=0.1):\n",
    "        # Epsilon-greedy: sometimes explore, sometimes exploit\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.q_eval.layers[-1].out_features)\n",
    "        else:\n",
    "            state_t = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                return int(torch.argmax(self.q_eval(state_t)).item())\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience into memory\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every few steps\n",
    "        self.step_counter = (self.step_counter + 1) % self.update_every\n",
    "        if self.step_counter == 0 and len(self.buffer) >= self.batch_size:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample a batch from memory\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        # Calculate targets\n",
    "        q_targets_next = self.q_target(next_states).detach().max(dim=1, keepdim=True)[0]\n",
    "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Calculate predictions\n",
    "        q_expected = self.q_eval(states).gather(1, actions)\n",
    "\n",
    "        # Minimize the loss\n",
    "        loss = self.loss_fn(q_expected, q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Slowly update the target network\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, eval_param in zip(self.q_target.parameters(), self.q_eval.parameters()):\n",
    "            target_param.data.copy_(self.tau * eval_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988e13-92f9-4333-a1e9-5f296f7d37dd",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='7'></a>\n",
    "## 7. Train the Agent\n",
    "This is like sending the agent to flight school.\n",
    "It practices landing over and over, learns from crashes, and slowly gets better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b72fe23d-5a20-435a-a255-ff698969b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, n_episodes=1500, max_steps=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    scores = []\n",
    "    epsilon = eps_start\n",
    "\n",
    "    for ep in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset(seed=ep)\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        epsilon = max(eps_end, eps_decay * epsilon)\n",
    "\n",
    "        if ep % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print(f\"Episode {ep} | Average (last 100 episodes): {avg_score:.2f}\")\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0085a3-a011-4d04-a423-b88919ccedb5",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='8'></a>\n",
    "## 8. Main Training Execution\n",
    "Here’s where we actually start the flight practice.\n",
    "We’ll let the agent fly thousands of times and keep track of how it’s doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6261f-6ca4-42b6-ab3a-bc8c0b959d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "# Train agent\n",
    "scores = train_dqn(env, agent)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(agent.q_eval.state_dict(), \"dqn_lander.pth\")\n",
    "\n",
    "# Close environment after training\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d577c7-45aa-4f5f-b172-1dc9731bd5d7",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='9'></a>\n",
    "\n",
    "## 9. Visualize Training Progress\n",
    "\n",
    "We’ll plot a simple graph to see how the agent’s landing skills improve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa8fc5-2380-437e-9595-23b28b1e7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Performance Over Episodes')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"training_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5378c89-19e2-4aa2-989c-be7cde6aa8d7",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='10'></a>\n",
    "## 10. Results, Interpretation, and Conclusion\n",
    "\n",
    "\n",
    "After training the DQN agent for 1500+ episodes, we observed the agent's total rewards improving over time. Although the agent has not consistently reached the success threshold of 200 points, the upward trend in rewards shows that it learned to land more safely and efficiently through practice.\n",
    "\n",
    "The agent started off crashing most of the time, but with experience replay and Q-learning, it gradually figured out how to slow down, stay balanced, and land closer to the pad. While there’s still room for better performance, the learning behavior and improvement curve clearly show that the model is working.\n",
    "\n",
    "With more training episodes, fine-tuned hyperparameters, or additional exploration strategies, the agent could likely achieve stable landings and even better scores.\n",
    "\n",
    "Overall, this project shows that Deep Q-Networks are a powerful approach to teaching an agent how to solve complex control tasks like landing a spacecraft.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930583a-0c83-4bc7-8960-76fb7604d1b6",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='11'></a>\n",
    "## 11. References\n",
    "- [Gymnasium LunarLander-v3 Documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "- [Deep Q-Learning Paper (Mnih et al., 2015)](https://arxiv.org/abs/1312.5602)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "- [Gymnasium GitHub Repository](https://github.com/Farama-Foundation/Gymnasium)\n",
    "- [Original Lunar Lander Reinforcement Learning Guide by sokistar24](https://github.com/sokistar24/Deep_Reinforcement_learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066508d-1de2-47e3-ab40-1d1832847008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
